---
layout: post
title: "Hypothesengetriebene Data Science: Klein anfangen - größer durchstarten"
author: "daten.company"
categories: insights
tags: [ki, mittelstand, kmu, crisp-dm]
image: /assets/img/screen.jpg
language: de
---

Als Unternehmen haben Sie sich entschieden datengetrieben zu werden -
Sie möchten Methoden und Ansätze aus den Bereichen Data Science, Machine Learning
und Künstlicher Intelligenz anwenden um u.a. Prozesse zu automatisieren,
Ihre Kunden gezielter anzusprechen oder Ihre Fertigung zu optimieren.

Nun stellt sich die Frage: Wie anfangen?

Welche Big Data Plattform benötigen Sie?
Benötige ich einen Data Lake?
Welcher Cloud Anbieter kann mir diese Plattform liefern?
Wie sieht es in der Cloud mit der Sicherheit meiner Daten aus?
Welche As-a-Service KI Produkte der großen Anbieter Google, Apple, Azure, Amazon AWS benötige ich?
Was kosten mich Big Data Lake und KI-as-a-Service meines Cloud Anbieters?
Und: Wie verknüpfe ich all das mit meinen internen Prozessen, Datenbanken und Fertigungslinien?

Wenn Sie von der initialen Entscheidung ein datengetriebenes Unternehmen werden zu wollen
direkt in die obigen Fragen einsteigen, legen Sie sich größere Steine in den Weg als notwendig ist.

Intelligenz und Insights aus Daten zu ziehen ist ein iterativer Prozess welcher hypothesengetrieben ist - ganz wie die Forschung:

Wir etablieren initial den Nutzen und Kontext aus Unternehmenssicht (Business Understanding): Könnten wir die finale Produktqualität anhand der eingestellten Parameter der Fertigungsstraße vorhersagen, dann wäre es uns bereits frühzeitig in der Herstellung einer Komponente möglich bei Problemen einzugreifen und gegenzusteuern.

Die Vorhersage ob die finale Qualität der aktuell in der Fertigungsstraße befindlichen Komponente ausreichend sein wird, ist im Bereich der künstlichen Intelligenz bzw. Machine Learning ein Klassifikationsproblem.

Hierfür betrachten wir als nächstes ob wir die richtigen Datentypen sammeln (Data Understanding):
Im Falle der Produktqualität benötigen wir bspw. je gefertigter Komponente alle initialen Metadaten (welcher Rohstoff wurde verwendet), alle durchlaufenen Fertigungsschritte und deren eingestellten Parameter, die final bewertete Qualität sowie externe Faktoren wie Temperatur und Luftfeuchtigkeit in der Fertigungshalle.

Sobald wir entschieden haben dass wir die richtigen Datentypen für diese Fragestellung sammeln ist die nächste Frage: Ist unsere Datenqualität ausreichend, bzw. beinhalten unsere gesammelten Daten statistische Zusammenhänge sodass eine Vorhersage der finalen Produktqualität möglich ist?

Diese Frage kann man erst beantworten wenn man mit der Modellierung beginnt (Data Preparation und Modeling) und anhand der gesammelten Daten anfängt die Vorhersagekraft gelernter Vorhersagemodelle abzuschätzen.

Diese Schritte - Data Preparation und Modeling - und unsere Erkenntnisse aus ihnen sind ausschlaggebend dafür ob wir uns mit den anfangs genannten Fragestellungen bezüglich Big Data, Data Lakes und Cloud Anbietern überhaupt auseinandersetzen müssen.
Diese essentiellen Schritte werden auch in aller Regel lokal auf gewöhnlicher Hardware (Laptop, Desktop) und unter Verwendung kostenloser Open Source Projekte durchgeführt - hier ist also in aller Regel kein Data Lake und keine Big Data Plattform notwendig.

Sollten wir hier sehen dass wir ein ausreichend gutes Vorhersagemodell erstellen können dann möchten wir natürlich als nächsten Schritt unsere Fertigungsstraße intelligent optimieren - hier sind dann Fragen bezüglich der Skalierung zu großen Datenmengen, Data Lakes, usw. wichtig.

Sollten wir jedoch sehen dass wir keine statistischen Zusammenhänge in unseren Daten erkennen dann können wir zur nächsten Fragestellung (Business Understanding) oder dem nächsten Topf Daten (Data Understanding) übergehen und so ressourcenschonend Hypothesen testen bis wir einen KI-Use Case finden welcher unser Unternehmen wertschöpfend voranbringt.

Dieser Ansatz schont Ressourcen und konzentriert unsere Ressourcen zu Beginn auf die einzig wichtige Frage: Gibt es statistische Zusammenhänge in unseren Daten welche wir wertschöpfend nutzen können?

Dieser hypothesengetriebene Ansatz wurde als Cross-Industry Standard Process for Data Mining (CRISP-DM) formalisiert:

{% include image.html
link="https://commons.wikimedia.org/wiki/File:CRISP-DM_Process_Diagram.png#/media/File:CRISP-DM_Process_Diagram.png"
caption="https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining"
%}
